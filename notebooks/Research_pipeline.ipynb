{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Pure Quantitative Trading Strategy\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook serves as a hands-on walkthrough of the end-to-end process for developing a **purely quantitative trading strategy**, inspired by the methodology outlined in *\"[Still in Development.]\"* The strategy design follows a well-defined, systematic framework designed to streamline the development pipeline.\n",
    "\n",
    "As with other experiments in this repository, we leverage **Mini_lib** ‚Äî our in-house research framework ‚Äî to simplify and modularize key steps in quantitative research applied to financial data. This notebook is structured around the following core stages:\n",
    "\n",
    "1. **Data Processing**  \n",
    "   This stage encompasses feature extraction, labeling, sampling, and data sanitization. Its primary goal is to uncover potential sources of alpha and exploitable patterns in raw market data ‚Äî essentially deciding *where to dig*.\n",
    "\n",
    "2. **Signal Extraction**  \n",
    "   Here we perform feature selection, model training, and hyperparameter tuning. This stage focuses on extracting predictive signals ‚Äî *using the right tools to dig efficiently*.\n",
    "\n",
    "3. **Strategy Tuning**  \n",
    "   This includes signal filtering, position sizing, and stop-loss rules. The goal is to refine raw signals into actionable trades ‚Äî *separating gold from dirt*.\n",
    "\n",
    "4. **Risk Management**  \n",
    "   This involves hedging and portfolio optimization to mitigate risk across assets and strategies ‚Äî *scaling the operation and protecting against downside*.\n",
    "\n",
    "5. **Backtesting**  \n",
    "   We run simulations and stress tests to evaluate the strategy's robustness and real-world viability ‚Äî *verifying if the mine is actually profitable*.\n",
    "\n",
    "By following this structured pipeline, the notebook aims to provide a clear, modular example of how to build and validate a fully systematic trading strategy ‚Äî from raw data to performance assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import Data as dt\n",
    "import ChevalParesseux_lib as lib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Getting Data and Making Samples**  \n",
    "\n",
    "The first step in our workflow is to **acquire and preprocess market data**. This involves:  \n",
    "\n",
    "1. **Fetching historical price data** ‚Äì Using reliable sources to obtain asset prices.  \n",
    "2. **Cleaning and preprocessing** ‚Äì Handling missing values, adjusting for corporate actions, and ensuring consistency.  \n",
    "   *(Note: The dataset used here is already processed for daily data, so we will skip this step.)*  \n",
    "3. **Creating research samples** ‚Äì Splitting the data into **Training**, **Testing**, and **Embargo** sets to prevent overfitting.  \n",
    "\n",
    "### ‚ö†Ô∏è **Important Note**  \n",
    "A **testing set** should be used **sparingly**! It is recommended to track the number of times it is accessed to prevent data leakage and ensure unbiased model evaluation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. Load data\n",
    "data = dt.load_data(ticker='A').drop(columns=['count_trx'])\n",
    "data.index = pd.to_datetime(data['date'])\n",
    "\n",
    "# II. Making Samples (we use a temporal sampling here)\n",
    "training_data = data.loc['2000-01-01':'2022-01-01'].copy()\n",
    "testing_data = data.loc['2022-01-01': '2024-10-01'].copy()\n",
    "embargo_data = data.loc['2024-10-01':].copy()\n",
    "\n",
    "# III. Let's see the training data we have on a log scale\n",
    "plt.figure(figsize=(17, 5))\n",
    "plt.plot(np.log(training_data['close']), label=training_data['code'].iloc[0])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Log Prices on Training Sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **I. Data Processing**\n",
    "\n",
    "### üéØ **Objective**  \n",
    "The objective of this stage is to prepare the dataset for effective model training by performing several key preprocessing steps:\n",
    "\n",
    "- **Feature Extraction**: Identify and compute features that may carry predictive power.\n",
    "- **Label Generation**: Create target variables if the task is supervised (classification or regression).\n",
    "- **Resampling**: Adjust the training data distribution to reflect financial constraints or modeling objectives (e.g., class imbalance, volatility targeting).\n",
    "- **Sanitization**: Clean and validate the extracted features and labels to ensure consistency and prevent downstream issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***1. Exploration of the data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The dataset used here follows a common structure for daily financial data analysis, \n",
    "containing the standard HLCV fields ‚Äî High, Low, Close, and Volume.  \n",
    "\n",
    "Notably, bid-ask spread information is not available, which means it will need to be implied or approximated \n",
    "during the backtesting phase to simulate more realistic execution conditions.\n",
    "\"\"\"\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's make some plots to deepen our understanding of the data and validate some assumptions.\n",
    "\n",
    "Analysis : \n",
    "\n",
    "- Returns distribution : Many of the financial time series are assumed to be normally distributed, however \n",
    "the empirical distribution of returns is often leptokurtic (fat-tailed) and skewed which is the case here.\n",
    "\n",
    "We'll have to take this into account when we build our model, usually a single model will capture either the \n",
    "extreme events or the normal events, but not both.\n",
    "\"\"\"\n",
    "# I. Plotting the returns series distribution\n",
    "training_data.loc[:, 'returns'] = training_data['close'].pct_change()\n",
    "\n",
    "lib.plot_series_distribution(series=training_data['returns'], title='Daily Close Returns Distribution')\n",
    "lib.plot_QQ(series=training_data['returns'], title='QQ Plot of Daily Close Returns Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- ACF and PACF : The autocorrelation function (ACF) and partial autocorrelation function (PACF) are used to\n",
    "determine the order of the ARIMA model. As we can see from the ACF and PACF plots, the returns series mostly\n",
    "display no significant autocorrelation, which is a common characteristic of financial time series. Still, some\n",
    "points are significant, indicating that there may be some patterns in the data that could be exploited.\n",
    "\"\"\"\n",
    "# II. Plotting the ACF and PACF\n",
    "lib.plot_acf(series=training_data['returns'], lags=50, title='ACF of Daily Close Returns Distribution')\n",
    "lib.plot_pacf(series=training_data['returns'], lags=50, title='PACF of Daily Close Returns Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Volatility clustering : The returns series exhibits volatility clustering, which is a common phenomenon in\n",
    "financial time series. This means that periods of high volatility are followed by periods of low volatility and vice versa.\n",
    "As we can see from the plot, the volatility of the returns series is not constant over time and tends to disminish over time.\n",
    "\"\"\"\n",
    "lib.plot_volatility(series=training_data['returns'], rolling_window=20, quantile=0.65, figsize=(17, 7), title='Volatility of Daily Close Returns Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***2. Labelling***\n",
    "\n",
    "### Labeling Strategy\n",
    "\n",
    "To maintain simplicity, we will later use a **Decision Tree Classifier** as our modeling algorithm. This choice requires the target variable to be represented as **discrete labels** rather than continuous values.\n",
    "\n",
    "These labels will also serve a dual purpose: they will help us **analyze and interpret the predictive power** of our features.\n",
    "\n",
    "For this purpose, we will employ the **Triple Barrier Labeling Method**, as introduced by **Marco L√≥pez de Prado**. This method provides a robust framework for labeling financial time series data by considering both profit-taking and stop-loss barriers, as well as a time-based exit condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= I. Set up the labeller =======\n",
    "labeller = lib.TripleBarrier_labeller()\n",
    "labeller_params = {\n",
    "    \"upper_barrier\": [1.5],\n",
    "    \"lower_barrier\": [1],\n",
    "    \"vertical_barrier\": [21],\n",
    "    \"vol_window\": [21],\n",
    "    \"smoothing_method\": ['ewma'],\n",
    "    \"window_smooth\": [5],\n",
    "    \"lambda_smooth\": [0.2],\n",
    "}\n",
    "labeller.set_params(**labeller_params)\n",
    "\n",
    "# ======= II. Extract the labels =======\n",
    "labels_df = labeller.extract(data=training_data['close'])\n",
    "training_data['label'] = labels_df[labels_df.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= III. Plot the labels =======\n",
    "focus_df = training_data.iloc[1000:1500].copy()\n",
    "lib.plot_series_labels(series=focus_df['close'], label_series=focus_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= V. Quick backtest =======\n",
    "\"\"\"\n",
    "Obviously, usings labels as signals will give insane results as the labels are based on the future data.\n",
    "Still, it's a nice verificatin of the labeller and the quick backtest function.\n",
    "\"\"\"\n",
    "lib.plot_quick_backtest(\n",
    "    series=focus_df['close'],\n",
    "    signal=focus_df['label'],\n",
    "    frequence='daily',\n",
    "    title='Quick Backtest of the Labels on the Series',\n",
    "    figsize=(17, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3. Features Extraction***\n",
    "\n",
    "### Modeling Approach\n",
    "\n",
    "To maintain simplicity in our initial model, we aim to capture the common patterns of **momentum**, as reflected in the **autocorrelation** of returns. This base model will focus on the central dynamics of return behavior.\n",
    "\n",
    "The more extreme behaviors ‚Äî particularly the **tails of the return distribution** ‚Äî will be addressed separately through a **meta-model** conditioned on **volatility regimes**. This separation allows us to isolate the core signal from the higher-order risk characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the number of jobs to use for parallel processing\n",
    "n_jobs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= I. Moving Average Based Feature =========\n",
    "avg_feature = lib.Average_feature(n_jobs=n_jobs)\n",
    "average_feature_params = {\n",
    "    \"window\": [5, 10, 20, 50],\n",
    "    \"smoothing_method\": [None, 'ewma'],\n",
    "    \"window_smooth\": [5],\n",
    "    \"lambda_smooth\": [0.2],\n",
    "}\n",
    "avg_feature.set_params(**average_feature_params)\n",
    "\n",
    "average_feature_df = avg_feature.extract(data=training_data['close'])\n",
    "training_data = pd.concat([training_data, average_feature_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_plot = average_feature_df.columns[0]\n",
    "focus_df = training_data.iloc[1000:1500].copy()\n",
    "\n",
    "lib.plot_series_distribution(series=focus_df[feature_to_plot], title='Smoothed Moving Average Feature Distribution')\n",
    "lib.plot_QQ(series=focus_df[feature_to_plot], title='QQ Plot of Smoothed Moving Average Feature Distribution')\n",
    "lib.plot_acf(series=focus_df[feature_to_plot], lags=50, title='ACF of Smoothed Moving Average Feature Distribution')\n",
    "lib.plot_volatility(series=focus_df[feature_to_plot], rolling_window=20, quantile=0.65, figsize=(17, 7), title='Volatility of Smoothed Moving Average Feature Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= II. Momentum Based Feature =========\n",
    "momentum_feature = lib.Momentum_feature(n_jobs=n_jobs)\n",
    "momentum_feature_params = {\n",
    "    \"window\": [5, 10, 20, 50],\n",
    "    \"smoothing_method\": [None, 'ewma'],\n",
    "    \"window_smooth\": [5],\n",
    "    \"lambda_smooth\": [0.2],\n",
    "}\n",
    "momentum_feature.set_params(**momentum_feature_params)\n",
    "\n",
    "momentum_feature_df = momentum_feature.extract(data=training_data['close'])\n",
    "training_data = pd.concat([training_data, momentum_feature_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= III. Volatility Based Feature =========\n",
    "vol_feature = lib.Volatility_feature(n_jobs=n_jobs)\n",
    "vol_feature_params = {\n",
    "    \"window\": [5, 10, 20, 50],\n",
    "    \"smoothing_method\": [None, 'ewma'],\n",
    "    \"window_smooth\": [5],\n",
    "    \"lambda_smooth\": [0.2],\n",
    "}\n",
    "vol_feature.set_params(**vol_feature_params)\n",
    "\n",
    "vol_feature_df = vol_feature.extract(data=training_data['close'])\n",
    "training_data = pd.concat([training_data, vol_feature_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= IV. Plot the Features =========\n",
    "feature_to_plot_1 = average_feature_df.columns[0]\n",
    "feature_to_plot_2 = vol_feature_df.columns[0]\n",
    "feature_to_plot_3 = momentum_feature_df.columns[0]\n",
    "\n",
    "focus_df = training_data.iloc[1000:1500].copy()\n",
    "\n",
    "lib.plot_feature_vs_label(feature_series=focus_df[feature_to_plot_1], label_series=focus_df['label'], feature_name=feature_to_plot_1)\n",
    "lib.plot_feature_vs_label(feature_series=focus_df[feature_to_plot_2], label_series=focus_df['label'], feature_name=feature_to_plot_2)\n",
    "lib.plot_feature_vs_label(feature_series=focus_df[feature_to_plot_3], label_series=focus_df['label'], feature_name=feature_to_plot_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***4. Sampling***\n",
    "\n",
    "Using a **discrete labeller** such as the **triple-barrier method** introduces several **sampling biases** that must be addressed:\n",
    "\n",
    "- **Concurrent labels**: Overlapping events distort true label uniqueness.\n",
    "- **Imbalanced label distribution**: Can bias the model toward dominant classes.\n",
    "- **Unequal importance of observations**: Rare but extreme events are critical to model performance due to the heavy-tailed nature of financial returns.\n",
    "\n",
    "Even if a model performs well on average, **failing to capture rare high-impact events** can lead to **disappointing real-world performance**.\n",
    "\n",
    "To mitigate these issues, we implement a **resampling strategy** that aims to:\n",
    "\n",
    "- ‚úÖ **Balance the label distribution**  \n",
    "- ‚úÖ **Maximize label uniqueness**  \n",
    "- ‚úÖ **Increase the presence of high-information points** (e.g., large absolute returns)\n",
    "\n",
    "This ensures a more robust and representative training dataset, especially crucial in financial time series with fat tails and overlapping label signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_selector = lib.Temporal_uniqueness_selection(n_jobs=n_jobs, random_state=72)\n",
    "obs_selector_params = {\n",
    "    'label_column': ['label'],\n",
    "    'price_column': ['close'],\n",
    "    'n_samples': [5000],\n",
    "    'replacement': [True],\n",
    "    'vol_window': [21],\n",
    "    'upper_barrier': [1.5],\n",
    "    'vertical_barrier': [21],\n",
    "    'grouping_column': [None],\n",
    "}\n",
    "obs_selector.set_params(**obs_selector_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extraction of the datasets.\n",
    "\n",
    "The outputs may sound strange but it makes sense : The main list contains datasets for different parameters (this is done for \n",
    "multiple purposes, mainly because it allows to make montecarlo simulations to assess the sensitivity of a model to the sampling parameters).\n",
    "Each sublist contains avery dataframes used (here we use a unique dataframe for daily data and a unique asset but it could be multiple\n",
    "assets or intraday data split by date etc...).\n",
    "\"\"\"\n",
    "# 1. Extract the datasets\n",
    "datasets = obs_selector.extract(data=training_data)\n",
    "resampled_df = datasets[0][0]\n",
    "resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As we can see, the datasets are now balanced. Furthermore, we have more labels as we get close to today and each one reflect a quite important return.\n",
    "\"\"\"\n",
    "# 2. Overview of the datasets\n",
    "values_count = resampled_df['label'].value_counts()\n",
    "print(values_count)\n",
    "\n",
    "# 3. Plot the datasets\n",
    "sorted_df = resampled_df.sort_values(by='date').copy()\n",
    "sorted_df.index = pd.to_datetime(sorted_df['date'])\n",
    "\n",
    "lib.plot_series_labels(series=sorted_df['close'], label_series=sorted_df['label']) # Whole series\n",
    "\n",
    "sorted_df = sorted_df.loc['2010-01-01':'2018-01-01'].copy()\n",
    "lib.plot_series_labels(series=sorted_df['close'], label_series=sorted_df['label']) # Zoomed in series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **II. Signal Extraction**\n",
    "\n",
    "### üéØ **Objective**  \n",
    "The goal of this stage is to extract trading signals from the previously processed dataset by applying a structured modeling workflow. This step transforms engineered features into actionable predictive insights.:\n",
    "\n",
    "- **Feature Selection**: Identify and retain the most relevant features that contribute significantly to model performance..\n",
    "- **Predictor Definition**: Select and define the model used to extract the signal from the data. Two broad categories can be considered: Discretionary or Machine Learning.\n",
    "- **Model Tuning**: Optimize the chosen model‚Äôs hyperparameters to improve predictive performance and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***1. Feature Selection***\n",
    "\n",
    "Even though neural networks are theoretically capable of approximating any function (the Universal Approximation Theorem), in practice, they often inherit biases from the data they are trained on. \n",
    "\n",
    "The feature selection process plays a crucial role in mitigating this issue. By carefully selecting relevant input features, we can reduce the risk of overfitting to noisy, irrelevant, or redundant data. This helps the model generalize better to unseen data, leading to improved performance and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 1. Feature Selection Model -----\n",
    "selector = lib.Correlation_selector(n_jobs=n_jobs)\n",
    "selector_params = {\n",
    "    'correlation_threshold': [0.9],\n",
    "}\n",
    "selector.set_params(**selector_params)\n",
    "\n",
    "# ----- 2. Fitting Model -----\n",
    "non_features = ['date', 'code', 'exchange', 'open', 'high', 'low', 'close', 'volume', 'returns', 'label']\n",
    "features_df = resampled_df.drop(columns=non_features).copy()\n",
    "selector.fit(data=features_df)\n",
    "\n",
    "nb_features_dropped = len(selector.features_to_drop)\n",
    "print(f'Number of features dropped : {nb_features_dropped}')\n",
    "print(f'Features that share a correlation higher than the defined threshold with another one : \\n{selector.features_to_drop}')\n",
    "\n",
    "# ----- 3. Extracting the features -----\n",
    "training_df = selector.extract(data=resampled_df)\n",
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***2. Predictor Selection***\n",
    "\n",
    "In this experiment, we aim to keep things simple and intuitive. That's why we start with a basic yet insightful model: a **Decision Tree Classifier**.\n",
    "\n",
    "While Decision Trees are easy to interpret and fast to train, they often suffer from high variance and a tendency to overfit, especially when left unpruned. Additionally, they may struggle to capture complex, non-linear relationships in the data.\n",
    "\n",
    "However, based on our current feature set, which appears to exhibit quasi-linear or regime-like behavior with respect to the target variable, a Decision Tree can still offer meaningful insights and serve as a reasonable baseline.\n",
    "\n",
    "For improved performance and generalization, I recommend experimenting with more robust ensemble methods such as **Random Forests** or **XGBoost**, which tend to handle overfitting and complex relationships much more effectively.\n",
    "\n",
    "Notes : Also, the Decision Tree method is handmade ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As models can be computationally expensive, we don't compute the all parameters grid directly here but use a defined set of parameters.\n",
    "For this example we use an exhaustiv set of parameters but usually we would directly pass the model through the hyperparameter tuning process.\n",
    "\"\"\"\n",
    "# ----- 1. Predcitor Model -----\n",
    "predictor = lib.Tree_classifier(n_jobs=n_jobs, random_state=72) # 72o also known as the \"beer hand\" in poker, a lucky number\n",
    "predictor_params = {\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 3,\n",
    "    'min_samples_split': 20,\n",
    "    'min_samples_leaf': 20,\n",
    "    'max_features': None,\n",
    "}\n",
    "predictor.set_params(**predictor_params)\n",
    "\n",
    "# ----- 2. Fitting Model -----\n",
    "non_features = ['date', 'code', 'exchange', 'open', 'high', 'low', 'close', 'volume', 'returns', 'label']\n",
    "X = training_df.drop(columns=non_features).copy()\n",
    "y = training_df['label'].copy()\n",
    "predictor.fit(X_train=X, y_train=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 3. Plot the tree -----\n",
    "lib.plot_tree(node=predictor.root, feature_names=X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3. Predictor Tuning***\n",
    "\n",
    "This step focuses on tuning the predictor model. Nearly all machine learning models are defined by a set of **hyperparameters** that can significantly influence their performance. Finding the optimal combination of these hyperparameters is essential to maximize model effectiveness.\n",
    "\n",
    "A simple yet effective approach is to perform a **grid search** combined with **cross-validation**. This method allows us to systematically explore different hyperparameter combinations while using cross-validation to assess how well the model generalizes to unseen data.\n",
    "\n",
    "By doing so, we can identify the best-performing configuration and reduce the risk of overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 1. Tuner Model -----\n",
    "tuner = lib.Classifier_gridSearch(n_jobs=1, random_state=72)\n",
    "tuner_params = {\n",
    "    'random_search': False,\n",
    "    'n_samples': 0, # useless as we don't use random search\n",
    "}\n",
    "tuner.set_params(**tuner_params)\n",
    "\n",
    "# ----- 2. Fitting Model -----\n",
    "model = lib.Tree_classifier\n",
    "grid_universe = { # We keep a simple grid universe for the example\n",
    "    'criterion': ['gini', 'shannon'],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [20],\n",
    "    'min_samples_leaf': [20],\n",
    "    'max_features': [None],\n",
    "}\n",
    "criteria = 'accuracy'\n",
    "\n",
    "sorted_train_df = training_df.sort_values(by='date').copy()\n",
    "sorted_train_df.reset_index(drop=True, inplace=True)\n",
    "features_df = sorted_train_df.drop(columns=non_features).copy()\n",
    "target_vector = sorted_train_df['label'].copy()\n",
    "\n",
    "data = []\n",
    "for i in range(0, 5):\n",
    "    idx = i * 1000\n",
    "    X_fold = sorted_train_df.iloc[idx:idx + 1000].drop(columns=non_features).copy()\n",
    "    y_fold = sorted_train_df.iloc[idx:idx + 1000]['label'].copy()\n",
    "    data.append((X_fold, y_fold))\n",
    "\n",
    "tuner.fit(model=model, grid_universe=grid_universe, data=data, criteria=criteria)\n",
    "best_params = tuner.best_params\n",
    "print(f'Wwith a {criteria} of {tuner.best_score:.2f}, Best parameters : {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The cross-validation average score is not the best metric to use here as we have a temporal data and the model is trained on the past data.\n",
    "But it is still a good metric to use to assess the model performance.\n",
    "\"\"\"\n",
    "data_train = (features_df, target_vector)\n",
    "fitted_model = tuner.extract(model=model, data=data_train)\n",
    "\n",
    "lib.plot_tree(node=fitted_model.root, feature_names=X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 3. Quick Backtest -----\n",
    "quick_data = training_data.dropna().copy()\n",
    "quick_X = quick_data.drop(columns=non_features).copy()\n",
    "quick_y = quick_data['label'].copy()\n",
    "\n",
    "quick_data.loc[:, 'signal'] = fitted_model.predict(features_matrix=quick_X)\n",
    "quick_data = quick_data.iloc[500:1000].copy()\n",
    "\n",
    "lib.plot_series_labels(series=quick_data['close'],label_series=quick_data['signal'],)\n",
    "\n",
    "lib.plot_quick_backtest(\n",
    "    series=quick_data['close'],\n",
    "    signal=quick_data['signal'],\n",
    "    frequence='daily',\n",
    "    title='Quick Backtest of the Labels on the Series',\n",
    "    figsize=(17, 5),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
